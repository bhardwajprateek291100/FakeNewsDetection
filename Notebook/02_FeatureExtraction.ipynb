{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_FeatureExtraction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KG0lLM4Bn23q"},"source":["##**Mounting Google Drive**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MK4jc0atnfOk","executionInfo":{"status":"ok","timestamp":1652683311229,"user_tz":-330,"elapsed":32314,"user":{"displayName":"PRATEEK BHARDWAJ","userId":"06364148084191331582"}},"outputId":"c1b9cf7e-c010-48ab-86df-a042200ed67c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"H-CW8nUEnwLU"},"source":["##**Import Packages**"]},{"cell_type":"code","metadata":{"id":"me_HneP_pkP8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652683316464,"user_tz":-330,"elapsed":5245,"user":{"displayName":"PRATEEK BHARDWAJ","userId":"06364148084191331582"}},"outputId":"23a350a2-832b-4f3e-fb5d-6572eedf39fb"},"source":["!pip install vaderSentiment\n","import numpy as np\n","import pandas as pd\n","from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n","import spacy\n","from collections import Counter\n","from nltk.tokenize import word_tokenize, sent_tokenize"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting vaderSentiment\n","  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 25.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51 kB 11.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71 kB 12.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81 kB 13.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122 kB 13.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 125 kB 13.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n","Installing collected packages: vaderSentiment\n","Successfully installed vaderSentiment-3.3.2\n"]}]},{"cell_type":"markdown","source":["##**Loading Dataset**"],"metadata":{"id":"6RW7Q4-MJ3QT"}},{"cell_type":"code","source":["dataset = pd.read_csv(\"/content/drive/MyDrive/BTP/Files/dataset.csv\", encoding='utf-8')"],"metadata":{"id":"fbPUz4N7J-Iv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###*Name Entity Features Extraction*"],"metadata":{"id":"IDvucs4qvHjl"}},{"cell_type":"code","source":["class NERFeatureExtraction:\n","    def __init__(self, df, textColumnName):\n","        print(\"NER Feature Extraction Starts\")\n","        self.m_new_df = pd.DataFrame()\n","        self.m_df = df\n","        self.m_textColumnName = textColumnName\n","        self.m_NER = spacy.load(\"en_core_web_sm\")\n","        self.m_NER_features = [\"PERSON\",\"ORG\",\"FAC\",\"GPE\",\"NORP\",\"LOC\",\"PRODUCT\",\"EVENT\",\"WORK_OF_ART\",\"LAW\",\"LANGUAGE\",\n","                            \"DATE\",\"TIME\",\"PERCENT\",\"MONEY\",\"CARDINAL\",\"QUANTITY\",\"ORDINAL\"]\n","        self.NER()\n","        print(\"NER Feature Extraction Done\\n\")\n","        \n","    def GetDataFrame(self):\n","        return self.m_new_df\n","        \n","    def NER(self):\n","        ner = []\n","        for idx, row in self.m_df.iterrows():\n","            sentence = self.m_NER(row[self.m_textColumnName])\n","            dic = dict.fromkeys(self.m_NER_features,0)\n","            labels = [x.label_ for x in sentence.ents]\n","            dic.update(Counter(labels))\n","            ner.append(dic)\n","        ner_df = pd.DataFrame.from_dict(ner)\n","        self.m_new_df = ner_df\n"],"metadata":{"id":"xHXtstL0ug1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Extraction takes time, Run only if have time\n","\n","dataset_NER = NERFeatureExtraction(dataset, 'statement').GetDataFrame()\n","dataset_NER.to_csv('/content/drive/MyDrive/BTP/Files/dataset_NER.csv', encoding='utf-8', index = False)"],"metadata":{"id":"_NUJUI4KnaJ4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###*Part of Speech Features Extraction*"],"metadata":{"id":"fI7dFZpTvOE-"}},{"cell_type":"code","source":["class POSTagFeatureExtraction:\n","    def __init__(self, df, textColumnName):\n","        print(\"POS Tag Feature Extraction Starts\")\n","        self.m_new_df = pd.DataFrame()\n","        self.m_df = df\n","        self.m_textColumnName = textColumnName\n","        self.m_POS = spacy.load(\"en_core_web_sm\")\n","        self.m_POS_features = [ \"ADJ\",\"ADP\",\"ADV\",\"AUX\",\"CCONJ\",\"DET\",\"INTJ\",\"NOUN\",\"NUM\",\"PART\",\n","                            \"PRON\",\"X\",\"PROPN\",\"PUNCT\",\"SCONJ\",\"SYM\",\"VERB\",\"SPACE\",\"CONJ\"]\n","        self.POS()\n","        print(\"POS Tag Feature Extraction Done\\n\")\n","        \n","    def GetDataFrame(self):\n","        return self.m_new_df\n","        \n","    def POS(self):\n","        pos_tag = []\n","        for idx, row in self.m_df.iterrows():\n","            sentence = self.m_POS(row[self.m_textColumnName])\n","            dic = dict.fromkeys(self.m_POS_features,0)\n","            labels = [x.pos_ for x in sentence]\n","            dic.update(Counter(labels))\n","            pos_tag.append(dic)\n","        pos_df = pd.DataFrame.from_dict(pos_tag)\n","        self.m_new_df = pos_df\n"],"metadata":{"id":"qTR-ilB1umBn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Extraction takes time, Run only if have time\n","\n","dataset_POSTag = POSTagFeatureExtraction(dataset, 'statement').GetDataFrame()\n","dataset_POSTag.to_csv('/content/drive/MyDrive/BTP/Files/dataset_POSTag.csv', encoding='utf-8', index = False)"],"metadata":{"id":"kyrbNcBUnwzf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###*Dependencies Features Extraction*"],"metadata":{"id":"w-KZrpdUvTtM"}},{"cell_type":"code","source":["class DependencyFeatureExtraction:\n","    def __init__(self, df, textColumnName):\n","        print(\"Dependency Feature Extraction Starts\")\n","        self.m_new_df = pd.DataFrame()\n","        self.m_df = df\n","        self.m_textColumnName = textColumnName\n","        self.m_Dep = spacy.load(\"en_core_web_sm\")\n","        self.m_Dep_features = self.m_Dep.pipe_labels['parser']\n","        self.Dependency()\n","        print(\"Dependency Feature Extraction Done\\n\")\n","        \n","    def GetDataFrame(self):\n","        return self.m_new_df\n","        \n","    def Dependency(self):\n","        dependencies = []\n","        for idx, row in self.m_df.iterrows():\n","            sentence = self.m_Dep(row[self.m_textColumnName])\n","            dic = dict.fromkeys(self.m_Dep_features,0)\n","            labels = [x.dep_ for x in sentence]\n","            labels = Counter(labels)\n","            for key in labels.keys():\n","                if key in dic:\n","                    dic[key] += labels[key]\n","            dependencies.append(dic)\n","        dependencies_df = pd.DataFrame.from_dict(dependencies)\n","        self.m_new_df = dependencies_df\n"],"metadata":{"id":"FqXS6-OdutOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Extraction takes time, Run only if have time\n","\n","dataset_Dependency = DependencyFeatureExtraction(dataset, 'statement').GetDataFrame()\n","dataset_Dependency.to_csv('/content/drive/MyDrive/BTP/Files/dataset_Dependency.csv', encoding='utf-8', index = False)"],"metadata":{"id":"I8cW8KYSNyS_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###*Sentiment Features Extraction*"],"metadata":{"id":"G8R7-ZAQvYFv"}},{"cell_type":"code","source":["class SentimentFeatureExtraction:\n","    def __init__(self, uncleaned, textColumnName):\n","        print(\"Sentiment Feature Extraction Starts\")\n","        self.m_analyzer = SentimentIntensityAnalyzer()\n","        self.m_new_df = pd.DataFrame()\n","        self.m_uncleaned = uncleaned\n","        self.m_textColumnName = textColumnName\n","        self.Sentiment()\n","        print(\"Sentiment Feature Extraction Done\\n\")\n","        \n","    def GetDataFrame(self):\n","        return self.m_new_df\n","    \n","    def Sentiment(self):\n","        sentiment = [self.m_analyzer.polarity_scores(text[self.m_textColumnName]) for idx,text in self.m_uncleaned.iterrows()]\n","        self.m_new_df = pd.DataFrame.from_dict(sentiment)\n","        self.m_new_df.drop(['compound'], axis='columns',inplace=True)"],"metadata":{"id":"YIvsRlatuJ6o","executionInfo":{"status":"ok","timestamp":1647351502327,"user_tz":-330,"elapsed":8,"user":{"displayName":"PRATEEK BHARDWAJ","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06364148084191331582"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c9f4a19b-4f90-4084-c4e1-d35512b4aed5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["## Extraction takes time, Run only if have time\n","\n","dataset_Sentiment = SentimentFeatureExtraction(dataset, 'statement').GetDataFrame()\n","dataset_Sentiment.to_csv('/content/drive/MyDrive/BTP/Files/dataset_Sentiment.csv', encoding='utf-8', index = False)"],"metadata":{"id":"vzQPRx1Vpdrm"},"execution_count":null,"outputs":[]}]}